import pandas

# Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. Удалите признаки,
# связанные с итогами матча (они помечены в описании данных как отсутствующие в тестовой выборке).
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import KFold, cross_val_score

data = pandas.read_csv('features.csv')
result_columns_names = ['duration', 'radiant_win', 'tower_status_radiant',
                        'tower_status_dire', 'barracks_status_radiant', 'barracks_status_dire']
result = data[result_columns_names]
data = data.drop(result_columns_names, axis=1)

# Проверьте выборку на наличие пропусков с помощью функции count(), которая для каждого столбца показывает число
# заполненных значений. Много ли пропусков в данных? Запишите названия признаков, имеющих пропуски, и попробуйте для
# любых двух из них дать обоснование, почему их значения могут быть пропущены.
# некоторые события не случились за 5 минут
# print(data.count()[data.count() != 97230])
# print(data.isnull().sum()[data.isnull().sum() != 0] )

# Замените пропуски на нули с помощью функции fillna(). На самом деле этот способ является предпочтительным для
# логистической регрессии, поскольку он позволит пропущенному значению не вносить никакого вклада в предсказание. Для
#  деревьев часто лучшим вариантом оказывается замена пропуска на очень большое или очень маленькое значение — в этом
#  случае при построении разбиения вершины можно будет отправить объекты с пропусками в отдельную ветвь дерева. Также
#  есть и другие подходы — например, замена пропуска на среднее значение признака. Мы не требуем этого в задании,
# но при желании попробуйте разные подходы к обработке пропусков и сравните их между собой.
data = data.fillna(0)  # TODO check another filling of NaN

# Какой столбец содержит целевую переменную? Запишите его название. radiant_win

# Забудем, что в выборке есть категориальные признаки, и попробуем обучить градиентный бустинг над деревьями на
# имеющейся матрице "объекты-признаки". Зафиксируйте генератор разбиений для кросс-валидации по 5 блокам (KFold),
# не забудьте перемешать при этом выборку (shuffle=True), поскольку данные в таблице отсортированы по времени,
# и без перемешивания можно столкнуться с нежелательными эффектами при оценивании качества. Оцените качество
# градиентного бустинга (GradientBoostingClassifier) с помощью данной кросс-валидации, попробуйте при этом разное
# количество деревьев (как минимум протестируйте следующие значения для количества деревьев: 10, 20, 30). Долго ли
# настраивались классификаторы? Достигнут ли оптимум на испытанных значениях параметра n_estimators, или же качество,
#  скорее всего, продолжит расти при дальнейшем его увеличении?

trees_count = [10, 20, 30]
for index, trees in enumerate(trees_count):
    clf = GradientBoostingClassifier(verbose=1, n_estimators=trees)
    clf.fit(data, result['radiant_win'])
    kf = KFold(n_splits=5, shuffle=True)
    cvs = cross_val_score(estimator=clf, cv=kf, X=data, y=result['radiant_win'])
    print('Trees = ', trees, 'Cross value score = ', cvs.mean())

# print(data['match_id'].value_counts())
